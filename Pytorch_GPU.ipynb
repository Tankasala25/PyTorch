{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNaVVBQYBzZ8qAz95oGnVpG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tankasala25/PyTorch/blob/main/Pytorch_GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Ways to Use GPUs for Deep Learning / PyTorch\n",
        "\n",
        "1.Free cloud notebooks: Google Colab or Kaggle Notebooks (no setup, good for learning).\n",
        "\n",
        "2.Cloud GPU platforms: AWS, Azure, GCP, RunPod, Paperspace, Lambda Labs (for training or production).\n",
        "\n",
        "3.Local GPU: Your own NVIDIA desktop/laptop or Apple Silicon (MPS backend)."
      ],
      "metadata": {
        "id": "4cP9R1pZXh_w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Way to check which GPU we are using"
      ],
      "metadata": {
        "id": "kSIYuhgGYXjY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A81Pi9lsXJzY",
        "outputId": "41b96b5d-61e4-42c8-a0f7-13f0b53527da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Nov  5 23:39:09 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "liNM5gv-Ywrq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###way to check gpu have access to pytorch"
      ],
      "metadata": {
        "id": "BZqE08ZuZGsQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VdxVFMlIYyT3",
        "outputId": "ee419d08-acbc-45e3-d512-bc1918eb1bfe"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Chekcing GPU Name"
      ],
      "metadata": {
        "id": "yL2QtLcYZWa1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.get_device_name()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "dsVrP9IlZNkD",
        "outputId": "5e29cf0a-cfe4-485c-c1b9-e9705f11cc81"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tesla T4'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Setting up device-agnostic code\n"
      ],
      "metadata": {
        "id": "WSCLaOuYZyzE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "cJdHa_VlZyEM",
        "outputId": "c1edef50-cb09-419e-937c-6fcb1969f690"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x=torch.tensor([1,2,3])"
      ],
      "metadata": {
        "id": "35_UTBfAZb0f"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###How to move tensor to GPU?\n"
      ],
      "metadata": {
        "id": "3lDo_htoaohD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x.cuda()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lMqU_B1amQg",
        "outputId": "6f1a7f05-b4ad-4b4b-add3-972a22539cdc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 2, 3], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDeyi6_Ya0su",
        "outputId": "9181da1e-55e5-4d75-eafc-a704fefdb932"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Always check if GPU is available?"
      ],
      "metadata": {
        "id": "2gImLunAbIYQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x=torch.tensor([1,2,3])\n",
        "if torch.cuda.is_available():\n",
        "  x=x.cuda()\n",
        "else:\n",
        "  print(\"GPU not available\")\n"
      ],
      "metadata": {
        "id": "yRG251WubFmM"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "x=torch.tensor([1,2,3],device=device)\n",
        "x.device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7m3kJHMlb2wK",
        "outputId": "020cd5ac-53be-471a-9dd7-527b2e1deb90"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###NumPy is a CPU-based library — it doesn’t understand GPU memory or CUDA.\n",
        "\n",
        "So, if you try this:"
      ],
      "metadata": {
        "id": "naEDqtNSdPXK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#x.numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "x5PROz6YcMij",
        "outputId": "defd1c30-66a6-4228-81fe-75c512989589"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2577979712.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Back from GPU to CPU"
      ],
      "metadata": {
        "id": "hxo-SluMc1mC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numpy_array=x.cpu().numpy()\n",
        "numpy_array"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iV0Zw5LXcUKQ",
        "outputId": "2bc74a49-1613-46e9-c08a-54900a0977d6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 2, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DqRjz7_Cca0E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}